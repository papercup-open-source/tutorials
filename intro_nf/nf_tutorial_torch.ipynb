{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "nf-tutorial-torch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raphpapercup/tutorials/blob/master/nf_tutorial_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDi15jKbtEl2",
        "colab_type": "text"
      },
      "source": [
        "# Normalizing flows tutorial - Papercup\n",
        "Copyright 2020-present, Papercup Technologies Limited  \n",
        "All rights reserved\n",
        "\n",
        "Author: Raphael Lenain\n",
        "\n",
        "__This tutorial is \"translated\" from the Jax tutorial at ICML 2019, run by Eric Jang at Google.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2Mi1lXOtEl4",
        "colab_type": "text"
      },
      "source": [
        "## Necessary imports\n",
        "Please remember to activate using GPU on Colab. See this guide for help: https://jovianlin.io/pytorch-with-gpu-in-google-colab/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIIJkvD0tEl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import distributions\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from sklearn import cluster, datasets, mixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Use GPU on Colab.\n",
        "torch.cuda.set_device(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zQiQRWbtEl7",
        "colab_type": "text"
      },
      "source": [
        "### We use the noisy moons dataset from sklearn.datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4oXCh6ltEl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_samples = 2000\n",
        "\n",
        "# Define distribution. \n",
        "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
        "X, y = noisy_moons\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Plot.\n",
        "xlim, ylim = [-2, 2], [-2, 2]\n",
        "plt.scatter(X[:, 0], X[:, 1], s=10, color='red')\n",
        "plt.xlim(xlim)\n",
        "plt.title('Noisy two moons distribution')\n",
        "plt.ylim(ylim);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlYb3a5ltEl9",
        "colab_type": "text"
      },
      "source": [
        "Let's explore what sampling from $N(0, I)$ looks like and overlay it with the noisy two moons.  \n",
        "As you can see very well below, $N(0, I)$ and the noisy moons distributions are very different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "82Ct5OYftEl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function which samples from N(0, I)\n",
        "def sample_n01(N):\n",
        "    # Sample from a normal(0, 1) distribution.\n",
        "    D = 2\n",
        "    return np.random.normal(size = (N, D))\n",
        "\n",
        "# N(0, I)\n",
        "X_normal = sample_n01(1000)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X[:, 0], X[:, 1], s=1, color='red', alpha=1)\n",
        "plt.scatter(X_normal[:, 0], X_normal[:, 1], s=10, color='green', alpha=0.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdQx1tGKtEl_",
        "colab_type": "text"
      },
      "source": [
        "### Loglikelihood of the two moons data under the normal distribution.\n",
        "Under the two moons data, this data is clearly bimodal. This makes sense as there are two moons, right?  \n",
        "In particular, you can tell that modeling the noisy moons distribution with $N(0, I)$ is quite a poor choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_uDs8N4OtEmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In the case of this tutorial, this function takes as input a list of 2 dimensional points, \n",
        "# and returns the list of the loglikelihoods of these points under the N(0, I) distribution.\n",
        "def log_prob_n01(x):\n",
        "    # Evaluate log likelihood under the normal distribution.\n",
        "    return np.sum(- np.square(x) / 2 - np.log(np.sqrt(2 * np.pi)), axis=-1)\n",
        "\n",
        "# Plot\n",
        "plt.hist(log_prob_n01(X), bins=50)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbhsbRQxtEmB",
        "colab_type": "text"
      },
      "source": [
        "### For comparison, loglikelihood of the normal(0, I_2) distributed data\n",
        "You can see here what evaluating the loglikelihood of a dataset under its correct distribution should look like.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZauWNyotEmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the loglikelihood of the N(0, I) data under the N(0, I) distribution (\"correct\" model).\n",
        "plt.hist(log_prob_n01(X_normal), bins=50)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UW6WeRGtEmE",
        "colab_type": "text"
      },
      "source": [
        "## Define the model\n",
        "The math here does not matter too much. If you are interested in the math of this, I can refer you to [the original paper](https://arxiv.org/abs/1605.08803), RealNVP.  \n",
        "\n",
        "This is the blog post by the original editor of the tutorial in Jax from which I wrote this tutorial: https://blog.evjang.com/2018/01/nf2.html. It also explains the math of the `NVP` class below quite well. \n",
        "\n",
        "Because of this I will not describe the math extensively here. \n",
        "\n",
        "__Note that NVP stands for \"Non volume preserving\".__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8CjVvXutEmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NVP(nn.Module):\n",
        "    def __init__(self, flips, D=2):\n",
        "        super().__init__()\n",
        "        self.D = D\n",
        "        self.flips = flips\n",
        "        self.prior = distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
        "        self.shift_log_scale_fns = nn.ModuleList()\n",
        "        for _ in flips:\n",
        "            shift_log_scale_fn = nn.Sequential(\n",
        "                nn.Linear(1, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, D),\n",
        "            )\n",
        "            self.shift_log_scale_fns.append(shift_log_scale_fn)\n",
        "    \n",
        "    def forward(self, x, flip_idx):\n",
        "        # x is of shape [B, H]\n",
        "        flip = self.flips[flip_idx]\n",
        "        d = x.shape[-1] // 2\n",
        "        x1, x2 = x[:, :d], x[:, d:]\n",
        "        if flip:\n",
        "            x2, x1 = x1, x2\n",
        "        net_out = self.shift_log_scale_fns[flip_idx](x1)\n",
        "        shift = net_out[:, :self.D // 2]\n",
        "        log_scale = net_out[:, self.D // 2:]\n",
        "        y2 = x2 * torch.exp(log_scale) + shift\n",
        "        if flip:\n",
        "            x1, y2 = y2, x1\n",
        "        y = torch.cat([x1, y2], -1)\n",
        "        return y\n",
        "    \n",
        "    def inverse_forward(self, y, flip_idx):\n",
        "        flip = self.flips[flip_idx]\n",
        "        d = y.shape[-1] // 2\n",
        "        y1, y2 = y[:, :d], y[:, d:]\n",
        "        if flip:\n",
        "            y1, y2 = y2, y1\n",
        "        net_out = self.shift_log_scale_fns[flip_idx](y1)\n",
        "        shift = net_out[:, :self.D // 2]\n",
        "        log_scale = net_out[:, self.D // 2:]\n",
        "        x2 = (y2 - shift) * torch.exp(-log_scale)\n",
        "        if flip:\n",
        "            y1, x2 = x2, y1\n",
        "        x = torch.cat([y1, x2], -1)\n",
        "        return x, log_scale\n",
        "    \n",
        "    @staticmethod\n",
        "    def base_log_prob_fn(x):\n",
        "        return torch.sum(- (x ** 2) / 2 - np.log(np.sqrt(2 * np.pi)), -1)\n",
        "    \n",
        "    def base_sample_fn(self, N):\n",
        "        # sampler random normal(0, I)\n",
        "        x = self.prior.sample((N, 1)).cuda().squeeze(1)\n",
        "        return x\n",
        "        \n",
        "    def log_prob(self, y, flip_idx):\n",
        "        x, log_scale = self.inverse_forward(y, flip_idx)\n",
        "        # This comes from the jacobian. In this case the jacobian is simply the product of the scales,\n",
        "        # which becomes the sum of log scales in the loglikelihood.\n",
        "        ildj = - torch.sum(log_scale, -1)\n",
        "        return self.base_log_prob_fn(x) + ildj\n",
        "    \n",
        "    def sample_nvp_chain(self, N):\n",
        "        xs = []\n",
        "        x = self.base_sample_fn(N)\n",
        "        xs.append(x)\n",
        "        for i, _ in enumerate(self.flips):\n",
        "            x = self.forward(x, flip_idx=i)\n",
        "            xs.append(x)\n",
        "        return x, xs\n",
        "    \n",
        "    def log_prob_chain(self, y):\n",
        "        # Run y through all the necessary inverses, keeping track\n",
        "        # of the logscale along the way, allowing us to compute the loss.\n",
        "        temp = y\n",
        "        logscales = y.data.new(y.shape[0]).zero_()\n",
        "        for i, _ in enumerate(self.flips):\n",
        "            temp, logscale = self.inverse_forward(\n",
        "                temp, \n",
        "                flip_idx=len(self.flips) - 1 - i,\n",
        "            )\n",
        "            # One logscale per element in a batch per layer of flow.\n",
        "            logscales += logscale.squeeze(-1)\n",
        "        return self.base_log_prob_fn(temp) - logscales"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv6NEjshtEmG",
        "colab_type": "text"
      },
      "source": [
        "### Define your NVP object. \n",
        "We define our model. In order to define our model, we need to give it a hyperparameter: the number of layers.  \n",
        "The number of layers is the length of flips. For reasons explained well in the blog post by Eric Jang, they have to alternate a function, which is why there is this __False/True/...__ structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-Ot8BNqtEmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flips = [False, True, False, True, False, True]\n",
        "\n",
        "# Feel free to try different flips lists. Eg:\n",
        "\n",
        "#  (More lightweight model, but less model capacity. Does not work very well.)\n",
        "# flips = [False, True] \n",
        "\n",
        "#  (Intermediary model. Works decently but fails to model the noisy moons properly.)\n",
        "# flips = [False, True, False, True] \n",
        "\n",
        "#  (Heavier model. Works well but is quite heavy. Might overload the GPU.)\n",
        "# flips = [False, True, False, True, False, True, False, True] \n",
        "\n",
        "model = NVP(flips).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAskSLUXtEmI",
        "colab_type": "text"
      },
      "source": [
        "### Loglikelihood of the two moons data under this new distribution.\n",
        "We have a new distribution which is computed using the $N(0, I)$ distribution and the model which we have just defined.\n",
        "These are still quite low and does not have the same structure as what we saw when we evaluated the $N(0, I)$ data under the $N(0, I)$ distribution.  \n",
        "\n",
        "__This makes sense, the model hasnt been trained yet.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fsywkwQBtEmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loglikelihoods = model.log_prob_chain(torch.FloatTensor(X).cuda()).data.cpu().numpy()\n",
        "plt.hist(loglikelihoods)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfz43HD3tEmK",
        "colab_type": "text"
      },
      "source": [
        "## Training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ZZNVufldtEmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training hyperparameters.\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Increase or decrease this if you wish.\n",
        "iters = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vA4qd9CwtEmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_enum = range(iters - 1)\n",
        "\n",
        "# Initialise the minimum loss at infinity.\n",
        "min_loss = float('inf')\n",
        "\n",
        "# Iterate over the number of iterations.\n",
        "for i in train_enum:\n",
        "    # Sample from our \"dataset\". We artificially have infinitely many data points here.\n",
        "    noisy_moons = datasets.make_moons(n_samples=128, noise=.05)[0].astype(np.float32)\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    batch = torch.FloatTensor(noisy_moons).cuda()\n",
        "    out_forwardpass = model.log_prob_chain(batch)\n",
        "    loss = - torch.mean(out_forwardpass)\n",
        "    # If the loss is lower than anything already encountered, consider that the \"best model\".\n",
        "    if loss.item() < min_loss:\n",
        "        bestmodel = model\n",
        "    \n",
        "    # Backpropagation.\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % 500 == 0:\n",
        "        print('Iter {}, loss is {:.3f}'.format(i, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXRlmM53tEmO",
        "colab_type": "text"
      },
      "source": [
        "### We have trained our model. Now we \"sample the NVP chain\".\n",
        "What does sampling the NVP chain mean? Well, we sample from $N(0, I)$, and apply the sequence of learned transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bMCL31nRtEmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_Xs, _ = bestmodel.sample_nvp_chain(10000)\n",
        "new_Xs = new_Xs.data.cpu().numpy()\n",
        "\n",
        "# Plot.\n",
        "plt.scatter(new_Xs[:, 0], new_Xs[:, 1], c='r', s=1)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUhKg_yDtEmQ",
        "colab_type": "text"
      },
      "source": [
        "### Make that GIF now\n",
        "Now we will make a pretty visualisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFohnh0CtEmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Necessary imports for this part.\n",
        "\n",
        "from matplotlib import animation, rc\n",
        "from IPython.display import HTML, Image\n",
        "\n",
        "def animate(i):\n",
        "    l = i//48\n",
        "    t = (float(i%48))/48\n",
        "    y = (1-t)*xs_list[l] + t*xs_list[l+1]\n",
        "    paths.set_offsets(y)\n",
        "    return (paths,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DCa2obNtEmS",
        "colab_type": "text"
      },
      "source": [
        "Run the cell below to plot images of the intermediary outputs.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7VPbU8LtEmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sample the NVP chain, and keep track of the intermediary outputs between each layer in the sequence.\n",
        "new_Xs, xs_list = bestmodel.sample_nvp_chain(10000)\n",
        "new_Xs = new_Xs.data.cpu().numpy()\n",
        "xs_list = [x.data.cpu().numpy() for x in xs_list]\n",
        "\n",
        "# Plot initial sample from N(0, I)\n",
        "plt.scatter(xs_list[0][:, 0], xs_list[0][:, 1], c='r', s=1)\n",
        "plt.title('Initial sample from N(0, I)')\n",
        "plt.show()\n",
        "\n",
        "# Plot intermediary outputs.\n",
        "for i, x in enumerate(xs_list[1:]):\n",
        "    plt.scatter(x[:, 0], x[:, 1], c='r', s=1)\n",
        "    plt.title('After layer {}'.format(i + 1))\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdBYaG13tEmU",
        "colab_type": "text"
      },
      "source": [
        "This cell also generates a GIF at `MyFunGif.gif` which you can download from the tab on the left in \"Files\" and look at.  \n",
        "__The cell below might take a while!!__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "yhw9Ol2MtEmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make the cool GIF image.\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xlim(-3, 3)\n",
        "ax.set_ylim(-3, 3)\n",
        "paths = ax.scatter(xs_list[0][:, 0], xs_list[0][:, 1], s=1, color='red')\n",
        "\n",
        "anim = animation.FuncAnimation(fig, animate, frames=48*len(flips), interval=1, blit=False)\n",
        "anim.save('MyFunGif.gif', writer='imagemagick', fps=60)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}